gpu: 0
seed: 42
n_workers: 1
output_dir: './tmp/'

train:
  batch_size: 4
  num_epochs: 10
  weight_decay: 0.01
  learning_rate: 5.0e-5  # The initial learning rate for Adam.
  warmup_steps: 100
  adam_epsilon: 1.0e-8  # Epsilon for Adam optimizer.
  gradient_accumulation_steps: 1  # Number of updates steps to accumulate before performing a backward/update pass

model:
  name: 'bert-base-uncased'
  use_pretrain: ''
  output_size: 512
  dropout_rate: 0.1
  hidden_size: 512
  hidden_act: 'gelu'
  initializer_range: 0.02
  vocab_size: 30522
  hidden_dropout_prob: 0.1
  num_attention_heads: 8
  type_vocab_size: 2
  max_position_embeddings: 512
  num_hidden_layers: 4
  intermediate_size: 2048
  attention_probs_dropout_prob: 0.0

data:
  dataset_path: './datasets/SemEval2010_task8/'
  train_data_file_name: 'TRAIN_FILE.TXT'
  test_data_file_name: 'TEST_FILE_FULL.TXT'
  max_len: 60
