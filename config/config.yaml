gpu: 0
seed: 42
n_workers: 1
models_output_dir: ./models/
output_dir: ./tmp/
log_frequency: 10

train:
  batch_size: 16
  num_epochs: 15
  weight_decay: 0.1
  learning_rate: 1.0e-5  # The initial learning rate for Adam.
  warmup_steps: 500
  adam_epsilon: 1.0e-6  # Epsilon for Adam optimizer.
  max_grad_norm: 1.0  # Max gradient norm.
  gradient_accumulation_steps: 1  # Number of updates steps to accumulate before performing a backward/update pass
  eval_frequency: 10000
  eval_batch_size: 64

model:
#  name: DeepPavlov/rubert-base-cased
  name: bert-large-cased
  use_pretrain: ''
  output_size: 512
  dropout_rate: 0.25
  hidden_size: 512
  hidden_act: gelu
  initializer_range: 0.02
  vocab_size: 30522
  hidden_dropout_prob: 0.1
  num_attention_heads: 8
  type_vocab_size: 2
  max_position_embeddings: 512
  num_hidden_layers: 4
  intermediate_size: 2048
  attention_probs_dropout_prob: 0.0

data:
  dataset_path: ./datasets/prepared/WebNLG/
  max_len: 200
  use_predefined_split: True
  use_zero_shot_split: False
